{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Sentiment Analysis\n",
    "### Individual Assignment for 5508\n",
    "### HE Peiwei, 54471952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "This report is to use different models to do the sentiment analysis with the Amazon reviews data. The dataset is Digital Music reviews. I defines the reviews which scores over 4 as good products, and seperate the data into 3 sets, 80% as training set, 15% as evaluation set and 15% as testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data and define the categories\n",
    "reviews = pd.read_pickle(\"reviews.pickle\")\n",
    "products = pd.read_pickle(\"products.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5555991584</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It's hard to believe \"Memory of Trees\" came ou...</td>\n",
       "      <td>09 12, 2006</td>\n",
       "      <td>A3EBHHCZO6V2A4</td>\n",
       "      <td>Amaranth \"music fan\"</td>\n",
       "      <td>Enya's last great album</td>\n",
       "      <td>1158019200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5555991584</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A clasically-styled and introverted album, Mem...</td>\n",
       "      <td>06 3, 2001</td>\n",
       "      <td>AZPWAXJG9OJXV</td>\n",
       "      <td>bethtexas</td>\n",
       "      <td>Enya at her most elegant</td>\n",
       "      <td>991526400</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  5555991584  [3, 3]      5.0   \n",
       "1  5555991584  [0, 0]      5.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  It's hard to believe \"Memory of Trees\" came ou...  09 12, 2006   \n",
       "1  A clasically-styled and introverted album, Mem...   06 3, 2001   \n",
       "\n",
       "       reviewerID          reviewerName                   summary  \\\n",
       "0  A3EBHHCZO6V2A4  Amaranth \"music fan\"   Enya's last great album   \n",
       "1   AZPWAXJG9OJXV             bethtexas  Enya at her most elegant   \n",
       "\n",
       "   unixReviewTime  good  \n",
       "0      1158019200  True  \n",
       "1       991526400  True  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['good'] = reviews.overall > 4\n",
    "#80% as training, 15% as evaluation, 15% as testing\n",
    "train = reviews.sample(frac=.7)\n",
    "evaluation = reviews.drop(train.index).sample(frac=.5)\n",
    "testing = reviews.drop(train.index).drop(evaluation.index)\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model Building\n",
    "## 2.1 Baseline Model\n",
    "Firstly, build a baseline moedel which simply assumes that the most frequent label will always be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict everything by the most frequently categories\n",
    "preditions_most=train.good.value_counts().idxmax()\n",
    "predictions_baseline=np.array(preditions_most).repeat(len(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real                  False  True \n",
      "predictions_baseline              \n",
      "True                   4340   5366\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.00      0.00      0.00      4340\n",
      "       Good       0.55      1.00      0.71      5366\n",
      "\n",
      "avg / total       0.31      0.55      0.39      9706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab(predictions_baseline, evaluation.good, rownames=['predictions_baseline'], colnames=['real']))\n",
    "print(metrics.classification_report(evaluation.good, predictions_baseline, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shown, the precision of this model by applying evaluation data is very low. And baseline model could only predict the good review as the scores for the bad reviews are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Initial Model\n",
    "Create a DTM of the review text, then use TF-IDF weighting to evaluate the importance of document, and use Naive Bayes model as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the DTM  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "dtm_train = count_vect.fit_transform(train.reviewText)\n",
    "dtm_evaluation = count_vect.transform(evaluation.reviewText)\n",
    "dtm_testing =count_vect.transform(testing.reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use TF-IDF weighting\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=True)\n",
    "dtm_train = tf_transformer.fit_transform(dtm_train)\n",
    "dtm_evaluation = tf_transformer.transform(dtm_evaluation)\n",
    "dtm_testing = tf_transformer.transform(dtm_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train NB model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_NB = MultinomialNB().fit(dtm_train, train.good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the categories\n",
    "predictions_initial = clf_NB.predict(dtm_evaluation)\n",
    "predictions_initial_testing = clf_NB.predict(dtm_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the evaluation set result of initial model\n",
      "real            False  True \n",
      "evaluation_set              \n",
      "False            1686    395\n",
      "True             2654   4971\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.81      0.39      0.53      4340\n",
      "       Good       0.65      0.93      0.77      5366\n",
      "\n",
      "avg / total       0.72      0.69      0.66      9706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Below are the evaluation set result of initial model\")\n",
    "print(pd.crosstab(predictions_initial, evaluation.good, rownames=['evaluation_set'], colnames=['real']))\n",
    "print(metrics.classification_report(evaluation.good, predictions_initial, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the testing set result of initial model\n",
      "real         False  True \n",
      "testing_set              \n",
      "False         1763    383\n",
      "True          2596   4964\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.82      0.40      0.54      4359\n",
      "       Good       0.66      0.93      0.77      5347\n",
      "\n",
      "avg / total       0.73      0.69      0.67      9706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Below are the testing set result of initial model\")\n",
    "print(pd.crosstab(predictions_initial_testing, testing.good, rownames=['testing_set'], colnames=['real']))\n",
    "print(metrics.classification_report(testing.good, predictions_initial_testing, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shown, the precision of bad reviews is 0.81, while the precision of good ones is 0.64, which means the Naive Bayes model could predict the bad reviews more precise than the good reviews, and the average performance is obvious better than the baseline model, as the precision is 0.72, f1-score is 0.65."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Improved Model\n",
    "### 2.3.1 Feature Dimension Reduction\n",
    "I've tried different classifiers such as KNN, SVM for the improvement, however the output is not very good. So I continue to do some improvement on the previous moedel. To improve the initial model, I conduct the improvement on the DTM, by reducing its dimensionality. After improved the DTM, I apply the same Navie Bayes model to predict the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature dimension reduction\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "ch2 = SelectKBest(chi2, k=1800)\n",
    "dtm_train = ch2.fit_transform(dtm_train, train.good)\n",
    "dtm_evaluation = ch2.transform(dtm_evaluation)\n",
    "dtm_testing = ch2.transform(dtm_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train NB model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_NB = MultinomialNB(alpha=1.2,fit_prior=False).fit(dtm_train, train.good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict the categories\n",
    "predictions_improved = clf_NB.predict(dtm_evaluation)\n",
    "predictions_improved_testing = clf_NB.predict(dtm_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the evaluation set result of improved NB model\n",
      "real            False  True \n",
      "evaluation_set              \n",
      "False            2929   1288\n",
      "True             1411   4078\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.69      0.67      0.68      4340\n",
      "       Good       0.74      0.76      0.75      5366\n",
      "\n",
      "avg / total       0.72      0.72      0.72      9706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "print(\"Below are the evaluation set result of improved NB model\")\n",
    "print(pd.crosstab(predictions_improved, evaluation.good, rownames=['evaluation_set'], colnames=['real']))\n",
    "print(metrics.classification_report(evaluation.good, predictions_improved, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the testing set result of improved NB model\n",
      "real         False  True \n",
      "testing_set              \n",
      "False         3041   1249\n",
      "True          1318   4098\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.71      0.70      0.70      4359\n",
      "       Good       0.76      0.77      0.76      5347\n",
      "\n",
      "avg / total       0.74      0.74      0.74      9706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Below are the testing set result of improved NB model\")\n",
    "print(pd.crosstab(predictions_improved_testing, testing.good, rownames=['testing_set'], colnames=['real']))\n",
    "print(metrics.classification_report(testing.good, predictions_improved_testing, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shown, the difference of the precisions on bad and good reviews has been reduced, while the overall precision has been improved to 0.72, and the f1-score is 0.72, which means feature dimension reduction is effective for improving the initial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 SVM - Grid Search\n",
    "I use grid search to find the best parameters for SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the parameters \n",
    "parameters = {'alpha': (0.00005,0.0001,0.001),\n",
    "              'average': (True, False),\n",
    "              'epsilon': (0.1, 0.2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set the classifer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_SVM_gs=SGDClassifier(loss='hinge',alpha=5e-05,learning_rate='optimal',class_weight={1: 1.2})\n",
    "gs_clf = GridSearchCV(clf_SVM_gs, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the classifer\n",
    "gs_clf = gs_clf.fit(dtm_train,train.good)\n",
    "predictions_gs = gs_clf.predict(dtm_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best error rate：0.75762794189075811\n",
      "alpha: 5e-05\n",
      "average: True\n",
      "epsilon: 0.2\n",
      "real            False  True \n",
      "predictions_gs              \n",
      "False            2657    729\n",
      "True             1683   4637\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.78      0.61      0.69      4340\n",
      "       Good       0.73      0.86      0.79      5366\n",
      "\n",
      "avg / total       0.76      0.75      0.75      9706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The best error rate：%r\" % (gs_clf.best_score_))\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "print(pd.crosstab(predictions_gs, evaluation.good, rownames=['predictions_gs'], colnames=['real']))\n",
    "print(metrics.classification_report(evaluation.good, predictions_gs, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shown, the best parameter for the SVM model is alpha=5e-05, so I apply this value to the model, fortunately, the precision seems to be a little higher than the improved naive bayes model, which is 0.75, while the f1-score is 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Personalized Prediction\n",
    "To personalize the prediction by using characteristics of the reviewer, I use the average score of the reviews of each reviewer and average score of each product as new feature, so as to predict the good and bad reviews by using SVM as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute the average score of the reviews of each reviewer on training set and apply to the whole data set\n",
    "index_average_reviewers=(pd.DataFrame(train.groupby('reviewerID').overall.mean()))\n",
    "index_average_reviewers.columns = ['average_reviewers']\n",
    "train=train.join(index_average_reviewers, on='reviewerID')\n",
    "evaluation=evaluation.join(index_average_reviewers, on='reviewerID')\n",
    "testing=testing.join(index_average_reviewers, on='reviewerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>good</th>\n",
       "      <th>average_reviewers</th>\n",
       "      <th>average_asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21061</th>\n",
       "      <td>B000002VS5</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is both Graham Parker's best album and a ...</td>\n",
       "      <td>03 22, 2008</td>\n",
       "      <td>A3KJ6JAZPH382D</td>\n",
       "      <td>Tim Brough \"author and music buff\"</td>\n",
       "      <td>Sparks create explosions</td>\n",
       "      <td>1206144000</td>\n",
       "      <td>True</td>\n",
       "      <td>3.976303</td>\n",
       "      <td>4.647059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin helpful  overall  \\\n",
       "21061  B000002VS5  [2, 2]      5.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "21061  This is both Graham Parker's best album and a ...  03 22, 2008   \n",
       "\n",
       "           reviewerID                        reviewerName  \\\n",
       "21061  A3KJ6JAZPH382D  Tim Brough \"author and music buff\"   \n",
       "\n",
       "                        summary  unixReviewTime  good  average_reviewers  \\\n",
       "21061  Sparks create explosions      1206144000  True           3.976303   \n",
       "\n",
       "       average_asin  \n",
       "21061      4.647059  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute the average score of the product of each product on training set and apply to the whole data set\n",
    "index_average_asin=(pd.DataFrame(train.groupby('asin').overall.mean()))\n",
    "index_average_asin.columns = ['average_asin']\n",
    "train=train.join(index_average_asin, on='asin')\n",
    "evaluation=evaluation.join(index_average_asin, on='asin')\n",
    "testing=testing.join(index_average_asin, on='asin')\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the DTM  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1),strip_accents='ascii',analyzer='word')\n",
    "dtm_train = count_vect.fit_transform(train.reviewText)\n",
    "dtm_evaluation = count_vect.transform(evaluation.reviewText)\n",
    "dtm_testing =count_vect.transform(evaluation.reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use TF-IDF weighting\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=True)\n",
    "dtm_train = tf_transformer.fit_transform(dtm_train)\n",
    "dtm_evaluation = count_vect.transform(evaluation.reviewText)\n",
    "dtm_testing = count_vect.transform(testing.reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_SVM=SGDClassifier(loss='hinge',alpha=0.001,learning_rate='optimal',class_weight={1: 1.2})\n",
    "clf_SVM.fit(train[['average_reviewers','average_asin']], train.good)\n",
    "# Predict the categories\n",
    "predictions_SVM = clf_SVM.predict(np.nan_to_num(evaluation[['average_reviewers','average_asin']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real             False  True \n",
      "predictions_SVM              \n",
      "False             2465   1141\n",
      "True              1906   4194\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.68      0.56      0.62      4371\n",
      "       Good       0.69      0.79      0.73      5335\n",
      "\n",
      "avg / total       0.69      0.69      0.68      9706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "print(pd.crosstab(predictions_SVM, evaluation.good, rownames=['predictions_SVM'], colnames=['real']))\n",
    "print(metrics.classification_report(evaluation.good, predictions_SVM, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shown, the precision is 0.68, and the f1-score is 0.68, which is a little lower than the improved naive bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1=\"Allison Krauss has an amazing voice.  Her band is good too.  Easy listening and easy to hum or sing along.\"\n",
    "b2=\"I like the Beach Boys, but I have to admit that I prefer their earlier California style hits (e.g., Fun, Fun, Fun) and I'm not a big fan of 60s music.  Nevertheless, \\\"Pet Sounds\\\" is undeniably a beautiful CD that still sounds great today.  \\\"Sloop John B\\\" and \\\"Wouldn't It Be Nice\\\" were top 10 hits in the US and \\\"God Only Knows\\\" and \\\"Caroline, No\\\" cracked the top 40, but all the songds sound great.  Brian Wilson's voice has never sounded more beautiful and the harmonies are near-perfect.  As a bonus, \\\"Hang on to Your Ego\\\" is included (it was recorded at the same time, but not released).  It's a great song and may be familiar to alt rock fans via a remake by Frank Black.  The liner notes are also terrific and include comments from Brian Wilson as well as a song by song description.  Great CD!\"\n",
    "b3=\"Allison Krauss has an amazing voice.  Her band is good too.  Easy listening and easy to hum or sing along.\"\n",
    "b4=\"Alison Krauss has one of the most beautiful voice.  Her band is really good too.  Easy listening.  Easy to hum or sing along.\"\n",
    "b5=\"Allison Krauss has a really amazing voice and sound.  Good band.  Easy listening, easy humming and easy to sing along.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "print(clf_NB.predict(ch2.transform(tf_transformer.transform(count_vect.transform([b1])))))\n",
    "print(clf_NB.predict(ch2.transform(tf_transformer.transform(count_vect.transform([b2])))))\n",
    "print(clf_NB.predict(ch2.transform(tf_transformer.transform(count_vect.transform([b3])))))\n",
    "print(clf_NB.predict(ch2.transform(tf_transformer.transform(count_vect.transform([b4])))))\n",
    "print(clf_NB.predict(ch2.transform(tf_transformer.transform(count_vect.transform([b5])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose 5 reviews from the training data, all of these 5 cases are predicted to be True as well as good, but their real catagoreis are bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct sentiment analysis on the Amazon review data, by using different classifiers to predict the good and bad product reviews. The initial model provides a simple prediction on the good reviews, but this couldn't work on the bad ones. The Navie Bayes as the initial model, provides better prediction and f1-score, however, the difference between good and bad reviews is quite large. To improve the naive bayes model, dimension reduction works effectively on this case, and successfully reduces the difference between good and bad predictions. After that, by using grid search, I find the best parameter for SVM, and get the highest precision as 0.74. Finally, I personalize the prediction by using characteristics of the reviewer also using the SVM, the precision is 0.68, which is close to the initial model.\n",
    "\n",
    "To conclude, if we want to get higher precision and f1-score, we could adjust the models by using different ways, such as TF-IDF weighting, changing the parameters, changing classifiers, so as to get a more precise prediction and acheive our goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
